{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每个格网的几何平均值或算术平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dls/data/openmmlab/test_RandomForestRegressor/mean/FULLER4D_5_mean.csv\n"
     ]
    }
   ],
   "source": [
    "outpath= '/home/dls/data/openmmlab/test_RandomForestRegressor/mean'\n",
    "path = '/home/dls/data/openmmlab/DGGRID/src/apps/caldgg/c'\n",
    "# proj = ['FULLER','ISEA']\n",
    "# topo = ['4D','4H','4T']\n",
    "proj = ['FULLER' ]\n",
    "topo = ['4D' ]\n",
    "res = 5\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "# from scipy.stats.mstats import gmean  \n",
    "for prj , tpo in product(proj,topo):\n",
    "    dirname = os.path.join(path,f\"{prj}{tpo}_{res}_all.csv\")\n",
    "    outname = os.path.join(outpath,f\"{prj}{tpo}_{res}_mean.csv\")\n",
    "    print(outname)\n",
    "    df =pd.read_csv(dirname)\n",
    "    # break\n",
    "    df.drop( columns=['seqnum'],inplace=True)\n",
    "    # print(df.columns)\n",
    "    # result = df.mean()\n",
    "    # # print(result)\n",
    "    # dfout =pd.DataFrame({f'{name}_mean':[result[i] ]for i, name in enumerate(df.columns) })\n",
    "    # print(dfout.head())\n",
    "    # # break\n",
    "    # dfout.to_csv(outname,index=False)\n",
    "    # df_gmean = np.log(df/result) \n",
    "    # print(df_gmean.sum())\n",
    "    # break\n",
    "    # 计算每一列的均值、方差、和标准差\n",
    "    means = df.mean()\n",
    "    cv =  df.std() / df.mean()##避免均值为0\n",
    "    stds = df.std()\n",
    "    # print(df.columns)\n",
    "    \n",
    "    # print(dfout.head())\n",
    "    # 创建一个新的DataFrame，其中每列的名称为以前列名加上相应的统计量\n",
    "    new_df = pd.DataFrame(columns=[col + '_' + stat for col in df.columns for stat in ['mean', 'var', 'std' ]])\n",
    "    # if df_gmean:\n",
    "    #     for col in df.columns:\n",
    "    #         new_df[col + '_ir'] = np.log(df[col]/df_gmean[col + '_gmean'])\n",
    "    # 将每个统计量添加到新的DataFrame中\n",
    "    for col in df.columns:\n",
    "        new_df[col + '_mean'] = [means[col]]\n",
    "        # new_df[col + '_cv'] = [cv[col]]\n",
    "        # new_df[col + '_std'] = [stds[col]]\n",
    "\n",
    "        # new_df[col + '_maxmin'] = df[col].max ()-df[col].min()#\n",
    "        # new_df[col + '_max'] = df[col].max ()\n",
    "        # new_df[col + '_min'] = df[col].min()\n",
    "    new_df.to_csv(outname,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每个格网的算术平均值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始计算错误了 把变异系数计算成了方差 进行纠正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out'\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "res =6\n",
    "import pandas as pd\n",
    "import os ,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.join(path,f\"{proj[0]}{topo[0]}_{res}_all.csv\")\n",
    "df =pd.read_csv(dirname)\n",
    "# 筛选包含 mean 或 std 的列 默认filter列筛选 想行筛选 坐标轴选择 axis=0\n",
    "columns_with_mean = df.filter(like='_mean').columns\n",
    "columns_with_std = df.filter(like='_std').columns\n",
    "\n",
    "#  遍历这些列，计算对应的 mean/std 的值\n",
    "# iteritems（） - 遍历列（键，值）对\n",
    "# iterrows（） - 遍历行（索引，序列）对\n",
    "for column_name, column_data in df[columns_with_mean].iteritems():#遍历每一列\n",
    "    stat_name = column_name.split('_')[0]  # 获取统计名称，如 area\n",
    "    std_column_name = f'{stat_name}_std'  # 构造对应的 std 列名，如 area_std\n",
    "    if std_column_name in columns_with_std:#如果有对应std名\n",
    "        std_column_data = df[std_column_name]  # 获取对应的 std 列数据\n",
    "        df[f'{stat_name}_cv'] = column_data / std_column_data  # 计算 mean/std 的值\n",
    "df = df.loc[:,~df.columns.str.contains('_var')]##loc 去除方差行 选择多列和多行\n",
    "df = df.loc[:,~df.columns.str.contains('_std')]##loc 去除标准差行\n",
    "df = df.loc[:,~df.columns.str.contains('_cv')]##loc 去除mean\n",
    "# 输出结果\n",
    "# print(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df[sorted(df.columns)]\n",
    "df_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop= df_sorted.drop(['name', 'r_lon', 'r_lat','recalls','accs','f1s'], axis=1 )\n",
    "pre =df_drop.pop('precisions')\n",
    "df_drop['precisions'] =pre\n",
    "df_drop.head()\n",
    "outpath = '/home/dls/data/openmmlab/test_RandomForestRegressor/processdata'\n",
    "dirname = os.path.join(outpath,f\"{proj[0]}{topo[0]}_{res}_process.csv\")\n",
    "df_drop.to_csv(dirname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_scaler(path,scaler):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "def load_scaler(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "        return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 读取原始数据\n",
    "df=df_drop\n",
    "\n",
    "# 对数据进行归一化 本身就是逐列归一化\n",
    "scaler = MinMaxScaler()\n",
    "scaler_path = os.path.join(outpath,f\"{proj[0]}{topo[0]}_{res}_scaler.pkl\")\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "save_scaler(scaler_path,scaler)\n",
    "sacler = load_scaler (scaler_path)\n",
    "# 将归一化后的数据再反归一化为原来的值\n",
    "df_denormalized = pd.DataFrame(scaler.inverse_transform(df_normalized), columns=df.columns)\n",
    "# 将 MinMaxScaler 对象保存到文件中\n",
    "\n",
    "# 打印结果\n",
    "print('Original data:\\n', df.head())\n",
    "print('Normalized data:\\n', df_normalized.head())\n",
    "print('Denormalized data:\\n', df_denormalized.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取在0 0 位置的 测试集精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "res =6\n",
    "json_path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out_0_0'\n",
    "json_dirname = os.path.join(json_path,f\"{proj[0]}{topo[0]}_l{res}_00.json\")\n",
    "json_dirname=json_dirname.lower()\n",
    "# 打开JSON文件并加载数据\n",
    "# 读取JSON文件\n",
    "with open(json_dirname, 'r') as file:\n",
    "    json_str = file.read()\n",
    "monitor ={'accs': 'accuracy/top1','precisions':'single-label/precision','recalls':'single-label/recall','f1s':'single-label/f1-score' }\n",
    "# 将JSON转换为字典\n",
    "jsondata = json.loads(json_str)\n",
    "# for key,value in monitor.items():\n",
    "#     current_score = jsondata[value]  \n",
    "dictval ={ key:[jsondata[value]] for key,value in monitor.items() }\n",
    "# 打印字典\n",
    "# print(dictval)\n",
    "dftestval =pd.DataFrame.from_dict(dictval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4H_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4H_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4T_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4T_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4D_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4D_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4H_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4H_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4T_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4T_6_all.csv\n"
     ]
    }
   ],
   "source": [
    "# 如果改变了统计量 需要重新合并/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out 里的 calstatis 和 metrics 文件变为all文件 与savepad功能相同\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out'\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "for prj , tpo in product(proj,topo):\n",
    "    \n",
    "    dirname_calstatis = os.path.join(path,f\"{prj}{tpo}_{res}_calstatis.csv\")\n",
    "    dirname_metircs = os.path.join(path,f\"{prj}{tpo}_{res}_metrics.csv\")\n",
    "    dirname_all = os.path.join(path,f\"{prj}{tpo}_{res}_all.csv\")\n",
    "    print(f'dirname: {dirname},{dirname_metircs}')\n",
    "    df_cal =pd.read_csv(dirname_calstatis)\n",
    "    df_met =pd.read_csv(dirname_metircs)\n",
    "    merged_df = pd.merge(df_cal, df_met, on='name', how='inner')\n",
    "    print(f'save result to {dirname_all}')\n",
    "    merged_df.to_csv(dirname_all,index=False)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理mmcls生成的数据 去除不需要的列 结果保存成 _process后缀的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out'\n",
    "path = '/home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out'\n",
    "outpath = '/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean'\n",
    "json_path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out_0_0'\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    " \n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "import json\n",
    "# 处理json数据为df文件\n",
    "def gettestscore(json_dirname):\n",
    "    with open(json_dirname, 'r') as file:\n",
    "        json_str = file.read()\n",
    "    monitor ={'accs': 'accuracy/top1','precisions':'single-label/precision','recalls':'single-label/recall','f1s':'single-label/f1-score' }\n",
    "    # 将JSON转换为字典\n",
    "    jsondata = json.loads(json_str)\n",
    "    # for key,value in monitor.items():\n",
    "    #     current_score = jsondata[value]  \n",
    "    dictval ={ key:[jsondata[value]] for key,value in monitor.items() }\n",
    "    # 打印字典\n",
    "    # print(dictval)\n",
    "    dftestval =pd.DataFrame.from_dict(dictval)\n",
    "    return dftestval\n",
    " \n",
    "for prj , tpo in product(proj,topo):\n",
    "    \n",
    "    dirname = os.path.join(path,f\"{prj}{tpo}_{res}_all.csv\")\n",
    "    print(f'dirname: {dirname}')\n",
    "    df =pd.read_csv(dirname)\n",
    "    # 筛选包含 mean 或 std 的列 默认filter列筛选 想行筛选 坐标轴选择 axis=0\n",
    "    columns_with_mean = df.filter(like='_mean').columns\n",
    "    columns_with_std = df.filter(like='_std').columns\n",
    "    #  遍历这些列，计算对应的 mean/std 的值\n",
    "    # iteritems（） - 遍历列（键，值）对\n",
    "    # iterrows（） - 遍历行（索引，序列）对\n",
    "    for column_name, column_data in df[columns_with_mean].items ():#遍历每一列\n",
    "        stat_name = column_name.split('_')[0]  # 获取统计名称，如 area\n",
    "        std_column_name = f'{stat_name}_std'  # 构造对应的 std 列名，如 area_std\n",
    "        if std_column_name in columns_with_std:#如果有对应std名\n",
    "            std_column_data = df[std_column_name]  # 获取对应的 std 列数据\n",
    "            df[f'{stat_name}_cv'] = std_column_data / column_data     # 计算 mean/std 的值\n",
    "    ##删除多余的变量\n",
    "\n",
    "    # df_drop= df_sorted.drop(['name', 'r_lon', 'r_lat','recalls','accs','f1s'], axis=1 )\n",
    "    df_drop= df.drop(['name', 'r_lon', 'r_lat' ], axis=1 )\n",
    "\n",
    "\n",
    "    # 调整 precisions 的位置\n",
    "    # preci =df_drop.pop('precisions')\n",
    "    # print(preci.head())\n",
    "    json_dirname = os.path.join(json_path,f\"{prj}{tpo}_l{res}_00.json\")\n",
    "    json_dirname=json_dirname.lower()\n",
    "    pre_test = gettestscore(json_dirname)#0 0 位置的预测精度\n",
    "\n",
    "    csv_dirname = os.path.join(json_path,\"out\",f\"{prj}{tpo}_{res}_00_calstatis.csv\") #0 0 位置处的格网几何属性\n",
    "    metric_test = pd.read_csv(csv_dirname)\n",
    "    metric_test= metric_test.drop(['name', 'r_lon', 'r_lat' ], axis=1 )\n",
    "    for key ,val in metric_test.items():##因为涉及负值 所以用变化率来表示 abs((a-b)/a)\n",
    "        value = float(metric_test[key]) \n",
    "        df_drop[f'{key}_norm'] = abs( (value-df_drop[key])/value ) ##几何属性数据标准化一下 存成对应的norm数据 利用的是 0 0 位置处进行标准化\n",
    "    for key ,val in pre_test.items():\n",
    "        value = float(pre_test[key]) \n",
    "        # print(key ,value)\n",
    "        df_drop[f'{key}_norm'] = abs( (value-df_drop[key])/value )##预测精度数据标准化一下 存成对应的norm数据 利用的是 0 0 位置处进行标准化\n",
    "\n",
    "    csv_mean = os.path.join('/home/dls/data/openmmlab/test_RandomForestRegressor/mean',f\"{prj}{tpo}_{res}_mean.csv\") #0 0 位置处的格网几何属性\n",
    "    metric_mean = pd.read_csv(csv_mean)\n",
    "    # metric_mean= metric_mean.drop(['name', 'r_lon', 'r_lat' ], axis=1 )\n",
    "    for key ,val in metric_mean.items():##因为涉及负值 所以用变化率来表示 abs((a-b)/a)\n",
    "        value = float(metric_mean[key]) \n",
    "        df_drop[f'{key}_meannorm'] = abs( (value-df_drop[key])/value ) ##用算术平均值norm存成对应的norm数据 利用的是全球位置的均值进行标准化\n",
    "        # 对数据进行归一化 本身就是逐列归一化\n",
    "        scaler = MinMaxScaler()\n",
    "        # print(df_drop[f'{key}_norm'])\n",
    "    # 根据计算的 mean_norm  计算mean_norm的std\n",
    "    # for column_name, column_data in df_drop.items ():#遍历每一列\n",
    "    #     if 'mean_norm' in column_name:##只有一个值了 无法计算std\n",
    "    #         stat_name = column_name.split('_')[0]  # 获取统计名称，如 area\n",
    "    #         df_drop[f'{stat_name}_meanstd'] =  column_data.std()     # 计算 mean/std 的值\n",
    "    # df_drop['precisions_norm'] = df_drop['precisions']/float(pre_test['precisions']) ##计算相应的归一化\n",
    "    # df_drop['f1s_norm']  = df_drop['f1s']/float(pre_test['f1s']) ##计算相应的归一化\n",
    "    # df_drop['accs_norm']  = df_drop['accs']/float(pre_test['accs']) ##计算相应的归一化\n",
    "    # df_drop['recalls_norm']  = df_drop['recalls']/float(pre_test['recalls']) ##计算相应的归一化\n",
    "    # print(pre_test['precisions'] )\n",
    "    # 再次排序\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('_var')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('_cv')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('max')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('min')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('maxmin')]##loc 去除方差行 选择多列和多行\n",
    "\n",
    "\n",
    "    # df = df.loc[:,~df.columns.str.contains('_std')]##loc 去除标准差行\n",
    "    # df = df.loc[:,~df.columns.str.contains('_cv')]##loc 去除mean\n",
    "    # df_sorted = df[sorted(df.columns)]\n",
    "    df_drop = df_drop[sorted(df_drop.columns)]\n",
    "    # print(preci_norm[preci_norm>0.8])\n",
    " \n",
    "    # print(df_drop.head())\n",
    "    # break\n",
    "    # 输出处理好的数据\n",
    "    # os.makedirs(outpath,   exist_ok=True)\n",
    "    outdirname = os.path.join(outpath,f\"{prj}{tpo}_{res}_process.csv\")\n",
    "    df_drop.to_csv(outdirname,index=False)\n",
    "    # print(df_drop.head())\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终合并所有的csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean/FULLER4T_6_process.csv\n",
      "(1800, 68)\n",
      "(1800, 68)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean/ISEA4H_6_process.csv\n",
      "(1800, 68)\n",
      "(3600, 68)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean/FULLER4D_6_process.csv\n",
      "(1800, 68)\n",
      "(5400, 68)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean/FULLER4H_6_process.csv\n",
      "(1800, 68)\n",
      "(7200, 68)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean/ISEA4T_6_process.csv\n",
      "(1800, 68)\n",
      "(9000, 68)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60_dggmean/ISEA4D_6_process.csv\n",
      "(1800, 68)\n",
      "(10800, 68)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 读取文件夹中所有 CSV 文件\n",
    "folder_path = outpath\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 创建空的 DataFrame\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# 循环遍历所有 CSV 文件\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    print(file_path)\n",
    "    # 读取 CSV 文件并将数据合并到 DataFrame 中\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.shape)\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df], axis=0)\n",
    "    print(merged_df.shape)\n",
    "merge_file = 'large_samples_b60_all_mean.csv'\n",
    "# 将合并后的 DataFrame 写入新的 CSV 文件\n",
    "merged_df.to_csv( merge_file , index=False)\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accs', 'accs_norm', 'angle_irmean', 'angle_irmean_norm', 'angle_irstd',\n",
       "       'angle_irstd_norm', 'angle_mean', 'angle_mean_meannorm',\n",
       "       'angle_mean_norm', 'angle_std', 'angle_std_meannorm', 'angle_std_norm',\n",
       "       'area_irmean', 'area_irmean_norm', 'area_irstd', 'area_irstd_norm',\n",
       "       'area_mean', 'area_mean_meannorm', 'area_mean_norm', 'area_std',\n",
       "       'area_std_meannorm', 'area_std_norm', 'csd_irmean', 'csd_irmean_norm',\n",
       "       'csd_irstd', 'csd_irstd_norm', 'csd_mean', 'csd_mean_meannorm',\n",
       "       'csd_mean_norm', 'csd_std', 'csd_std_meannorm', 'csd_std_norm',\n",
       "       'dis_irmean', 'dis_irmean_norm', 'dis_irstd', 'dis_irstd_norm',\n",
       "       'dis_mean', 'dis_mean_meannorm', 'dis_mean_norm', 'dis_std',\n",
       "       'dis_std_meannorm', 'dis_std_norm', 'f1s', 'f1s_norm', 'per_irmean',\n",
       "       'per_irmean_norm', 'per_irstd', 'per_irstd_norm', 'per_mean',\n",
       "       'per_mean_meannorm', 'per_mean_norm', 'per_std', 'per_std_meannorm',\n",
       "       'per_std_norm', 'precisions', 'precisions_norm', 'recalls',\n",
       "       'recalls_norm', 'zsc_irmean', 'zsc_irmean_norm', 'zsc_irstd',\n",
       "       'zsc_irstd_norm', 'zsc_mean', 'zsc_mean_meannorm', 'zsc_mean_norm',\n",
       "       'zsc_std', 'zsc_std_meannorm', 'zsc_std_norm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df.columns)\n",
    "merged_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据归一化测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor  ,GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 加载数据集\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/merge_all.csv'\n",
    "# data = pd.read_csv(path)\n",
    "data =merged_df\n",
    "# print(list(data.columns))\n",
    "# data_y_temp = ['recalls','accs','f1s','precisions']\n",
    "# data_y = []\n",
    "# for name in data_y_temp:\n",
    "#     data_y.append(name)\n",
    "#     data_y.append(name+'_norm')\n",
    "data_y=['recalls',\n",
    " 'recalls_norm',\n",
    " 'accs',\n",
    " 'accs_norm',\n",
    " 'f1s',\n",
    " 'f1s_norm',\n",
    " 'precisions',\n",
    " 'precisions_norm']\n",
    "x_name = [name for name in data.columns if name not in data_y]\n",
    " \n",
    "# data_x =  data.iloc[:, :-1]\n",
    "# data_y =  data.iloc[:,  -1]\n",
    "\n",
    "# # print(data_x.head())\n",
    "# # print(data_y.head())\n",
    "\n",
    "# # need_name = ['area','per','zsc','disminmax','disavg','angleminmax','angleavg','csdminmax','csdavg','precisions' ]\n",
    "# # data =data[need_name]\n",
    "# # 将数据集分成训练集和测试集 最后一列是因变量 剩余的列是自变量\n",
    "# scaler = MinMaxScaler()\n",
    "# df_normalized = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_normalized, data_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因子分析实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "# 计算巴特利特P值\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算相关关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angle_m', 'angle_s', 'area_m', 'area_s', 'csd_m', 'csd_s', 'dis_m',\n",
       "       'dis_s', 'per_m', 'per_s', 'zsc_m', 'zsc_s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/dls/data/openmmlab/test_RandomForestRegressor/large_samples_b60_all.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/large_samples_b60_all_mean.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4D_6_process.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4H_6_process.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4T_6_process.csv'\n",
    "# path = '/home/dls/data/openmmlab/test_RandomForestRegressor/merge_nopre_test.csv'\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "data = pd.read_csv(path)\n",
    "data_y=['recalls',\n",
    " 'recalls_norm',\n",
    " 'accs',\n",
    " 'accs_norm',\n",
    " 'f1s',\n",
    " 'f1s_norm',\n",
    " 'precisions',\n",
    " 'precisions_norm']\n",
    "x_name = [name for name in data.columns if name not in data_y]\n",
    "# print(x_name)\n",
    "data_x =  data[x_name]\n",
    "# print(data_x.columns)\n",
    "data_y =  data[data_y]\n",
    "# print(data_x.head())\n",
    "# print(data_y.head())\n",
    "\n",
    "# need_name = ['area','per','zsc','disminmax','disavg','angleminmax','angleavg','csdminmax','csdavg','precisions' ]\n",
    "# data =data[need_name]\n",
    "# 将数据集分成训练集和测试集 最后一列是因变量 剩余的列是自变量\n",
    "# scaler = MinMaxScaler() #为了使用同一个归一化器 先归一化 再分割\n",
    "# namefilter =[name for name in df_normalized.columns if (('norm'in name) and ('_ir' not in name)) ]\n",
    "namefilter =[name for name in data_x.columns if (('norm'in name) and ('_ir' not in name)) ]\n",
    "\n",
    "# namefilter =[name for name in data_x.columns if (('_meannorm'in name) and ('_ir' not in name))  ]\n",
    "# print(namefilter)\n",
    "data_x=data_x[namefilter]\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns)\n",
    "# df_normalized.head()\n",
    "\n",
    "# corr = data_x.corrwith(data_y['f1s_norm'] )\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_x_norm.txt','w') as f :\n",
    "#     # ind =corr.index.str.contains('_ir')\n",
    " \n",
    "#     # f.write(\n",
    "#     #     f'Correlation between data_x and f1s_norm :\\n{corr[~ind]}\\n'\n",
    "#     # )\n",
    "#     f.write(\n",
    "#         f'Correlation between data_x and f1s_norm :\\n{corr}\\n'\n",
    "#     )\n",
    "\n",
    "# 发现norm完后的结果普遍变好 利用spherephd处理的结果没有普通的好\n",
    "\n",
    "# corr = df_normalized.corrwith(data_y['f1s_norm'] )\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_norm.txt','w') as f :\n",
    "#     # ind =corr.index.str.contains('_ir')\n",
    "#     # out =corr[~ind]\n",
    "#     # ind1 = out.index.str.contains('norm')\n",
    "#     # out =out[ind1]\n",
    "#     # f.write(\n",
    "#     #     f'Correlation between df_normalized and f1s_norm :\\n{out}\\n'\n",
    "#     # )\n",
    "#     f.write(\n",
    "#         f'Correlation between df_normalized and f1s_norm :\\n{corr}\\n'\n",
    "#     )\n",
    "# 发现归一化与不归一化结果相同 但是归一化后 对随机森林方便验证\n",
    "\n",
    "#提取包含norm的值作为自变量\n",
    "\n",
    "\n",
    "# df_normalized = df_normalized[namefilter]##loc 去除方差行 选择多列和多行\n",
    "# newcolumns = []\n",
    "df_normalized_temp = pd.DataFrame()\n",
    "df_normalized_std =pd.DataFrame()\n",
    "df_normalized_mean =pd.DataFrame()\n",
    "# print(data_x.columns)\n",
    "for name ,value in df_normalized.items():\n",
    "    # print(name)\n",
    "    if 'std'in name :\n",
    "        base=name.split('_')[0]\n",
    "        newname = base+'_s'\n",
    "        df_normalized_temp[newname] =value\n",
    "        df_normalized_std[newname] = value\n",
    "    elif 'mean'in name :\n",
    "        base=name.split('_')[0]\n",
    "        newname = base+'_m'\n",
    "        df_normalized_temp[newname] =value\n",
    "        df_normalized_mean[newname] = value\n",
    "    else:\n",
    "        raise(f'name is {name}')\n",
    "df_normalized =df_normalized_temp\n",
    "df_normalized.columns\n",
    "# 发现norm完后的结果普遍变好 利用spherephd处理的结果没有普通的好\n",
    "# corr = df_normalized.corrwith(data_y['precisions_norm'] )\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_norm.txt','w') as f :\n",
    "#     f.write(f'Correlation between df_normalized and  precisions_norm  :\\n{corr}\\n')\n",
    "\n",
    "# corr = df_normalized.corrwith(data_y['precisions'] )\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_y.txt','w') as f :\n",
    "#     f.write(f'Correlation between df_normalized and  precisions  :\\n{corr}\\n')\n",
    "# for key ,val in data_y.items():\n",
    "#     corr = data_x.corrwith(data_y[key] )\n",
    "#     with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_x.txt','w') as f :\n",
    "#         f.write(f'Correlation between data_x and {key}:\\n{corr}\\n')\n",
    "#         # print(f'Correlation between x and {key}:', corr)\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm.txt','w+') as f :\n",
    "#     for key ,val in data_y.items():\n",
    "#         corr = df_normalized.corrwith(data_y[key] )\n",
    "#         f.write(f'Correlation between df_normalized and {key}:\\n{corr}\\n')\n",
    "        # print(f'Correlation between data_x and {key}:', corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 自变量之间的相关关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "          angle_m   angle_s    area_m    area_s     csd_m     csd_s     dis_m  \\\n",
      "angle_m  1.000000  0.587961  0.338260  0.005026  0.657503  0.559216  0.819589   \n",
      "angle_s  0.587961  1.000000  0.292988 -0.112494  0.308914  0.210005  0.526325   \n",
      "area_m   0.338260  0.292988  1.000000 -0.282309  0.368912  0.517256  0.242494   \n",
      "area_s   0.005026 -0.112494 -0.282309  1.000000 -0.011229  0.026784  0.039304   \n",
      "csd_m    0.657503  0.308914  0.368912 -0.011229  1.000000  0.917599  0.605029   \n",
      "csd_s    0.559216  0.210005  0.517256  0.026784  0.917599  1.000000  0.585372   \n",
      "dis_m    0.819589  0.526325  0.242494  0.039304  0.605029  0.585372  1.000000   \n",
      "dis_s    0.598086  0.592376 -0.067371  0.567386  0.368571  0.249398  0.495033   \n",
      "per_m    0.851967  0.481642  0.188804  0.057946  0.672018  0.607002  0.981545   \n",
      "per_s    0.632218  0.714285 -0.066313  0.236403  0.251831  0.176119  0.611839   \n",
      "zsc_m    0.889695  0.415588  0.076310  0.077152  0.699088  0.564731  0.878812   \n",
      "zsc_s    0.503422  0.561347  0.378832  0.177782  0.365602  0.468573  0.589342   \n",
      "\n",
      "            dis_s     per_m     per_s     zsc_m     zsc_s  \n",
      "angle_m  0.598086  0.851967  0.632218  0.889695  0.503422  \n",
      "angle_s  0.592376  0.481642  0.714285  0.415588  0.561347  \n",
      "area_m  -0.067371  0.188804 -0.066313  0.076310  0.378832  \n",
      "area_s   0.567386  0.057946  0.236403  0.077152  0.177782  \n",
      "csd_m    0.368571  0.672018  0.251831  0.699088  0.365602  \n",
      "csd_s    0.249398  0.607002  0.176119  0.564731  0.468573  \n",
      "dis_m    0.495033  0.981545  0.611839  0.878812  0.589342  \n",
      "dis_s    1.000000  0.512602  0.833356  0.522211  0.569499  \n",
      "per_m    0.512602  1.000000  0.587547  0.947046  0.532451  \n",
      "per_s    0.833356  0.587547  1.000000  0.540108  0.739144  \n",
      "zsc_m    0.522211  0.947046  0.540108  1.000000  0.379723  \n",
      "zsc_s    0.569499  0.532451  0.739144  0.379723  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 自变量之间的相关关系\n",
    "corr_matrix = df_normalized.corr()\n",
    "\n",
    "print('Correlation matrix:')\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10800, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10800, 12)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=True) # 启用LaTeX\n",
    "plt.rc('font', family='sans-serif') # 设置LaTeX字体\n",
    "sns.set(  font_scale=2.5 )\n",
    "def qqplot (x, y, **kwargs):\n",
    "   regax =sns.regplot(x=x,y=y,line_kws={\"color\": \"k\",\"linewidth\": 0.8, \"linestyle\": \"--\"}, scatter_kws={\"s\": 2   })##,marker=\"*\" , \"color\": \"k\"\n",
    "#    x_lim = (-0.1,1.0)\n",
    "#    y_lim = (0,1.0)\n",
    "#    regax.set_xlim(x_lim)\n",
    "#    regax.set_ylim(y_lim)\n",
    "   # regax.set_facecolor(\"none\")\n",
    "def hexbin(x, y, color, **kwargs):\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)\n",
    "def upper_regplot(x, y, **kwargs):\n",
    "      # rgb = np.array([216, 229, 210])/255.0\n",
    "      scaax =sns.kdeplot(x=x,y=y  )\n",
    "      # x_lim = (-0.1,1.1)\n",
    "      # y_lim = (-0.1,1.1)\n",
    "      # scaax.set_xlim(x_lim)\n",
    "      # scaax.set_ylim(y_lim)\n",
    "def generate_text(x,y, **kwargs):\n",
    "      # print(kwargs)\n",
    "      scaax = plt.gca()\n",
    "      # print(scaax .yaxis.get_label())\n",
    "\n",
    "      x_lim = (-0.05,1.1)\n",
    "      # scaax.set_xlim(x_lim)\n",
    "      # scaax.set_ylim(y_lim)\n",
    "      x_range = scaax.get_xlim()[1] - scaax.get_xlim()[0]\n",
    "      y_range = scaax.get_ylim()[1] - scaax.get_ylim()[0]\n",
    "      value = round(x.corr(y), 2)\n",
    "      if value >0.39:\n",
    "            color='r'\n",
    "            fontweight='heavy'\n",
    "      else:\n",
    "            color=None\n",
    "            fontweight='normal'\n",
    "\n",
    "      text = r\"$\\rho$\"+  f\"={round(x.corr(y), 2)}\"\n",
    "      # plt.gca().set_color(p1.get_color())\n",
    "      plt.text(x_lim[1]*0.65 ,x_lim[1]*0.65 , text, ha='center', va='center',fontsize =30,color=color,fontweight=fontweight)\n",
    "      \n",
    "# 定义一个函数用于在上半部分绘制文本\n",
    "# def plot_text(x,y,  **kwargs):\n",
    "#       # scaax=ax  \n",
    "#       # rgb = np.array([216, 229, 210])/255.0\n",
    "\n",
    "#       # sns.kdeplot(x=x,y=y,  levels=4, color=\".2\")\n",
    "#       r= np.random.randint(0,10)\n",
    " \n",
    "#       # x_range = scaax.get_xlim()[1] - scaax.get_xlim()[0]\n",
    "#       # y_range = scaax.get_ylim()[1] - scaax.get_ylim()[0]\n",
    "#       # x_lim = (-0.05,1.0)\n",
    "#       # y_lim = (-0.05,1.0)\n",
    "#       # scaax.set_xlim(x_lim)\\\n",
    "#       # scaax.set_ylim(y_lim)\n",
    "#       plt.text(0.5, 0.5, text, ha='center', va='center',fontsize = 2)\n",
    "     \n",
    "#       # scaax.patch.set_facecolor(rgb)\n",
    "#       # scaax.patch.set_alpha(0.50)\n",
    "#       # sns.despine(ax=scaax, left=True, bottom=True)\n",
    "# 在上半部分应用绘图函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10800,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_normalized['area_m'].shape)\n",
    "df_normalized['area_m'].hist(  bins=20, density=True, alpha=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关关系出图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"whitegrid\") # 设置seaborn样式\n",
    "# dataset = pd.concat([df_normalized.iloc[:,0:2],data_y['f1s_norm']],axis=1)\n",
    "y = data_y['f1s_norm']\n",
    "y=y.rename('f1')\n",
    "# print(y.name)\n",
    "dataset = pd.concat([df_normalized,y],axis=1)\n",
    "dataset_std = pd.concat([df_normalized_std,y],axis=1)\n",
    "dataset_mean= pd.concat([df_normalized_mean,y],axis=1)\n",
    "\n",
    "print(dataset.shape)\n",
    "def sactterplot(dataset,picname,x_vars,y_vars):\n",
    "    g = sns.PairGrid(dataset,x_vars=x_vars,y_vars=y_vars )\n",
    "    # g.map_upper(sns.regplot)\n",
    "    # reg =sns.regplot(marker=\"*\",line_kws={\"color\": \"C1\"})\n",
    "    g.map_lower(qqplot)\n",
    "    g.map_lower(generate_text)\n",
    "\n",
    "    x_lim = (-0.2,1.1)\n",
    "    y_lim = (-0.2,1.1)\n",
    "    # g.map_upper(upper_regplot)\n",
    "    g.set(xticks=[0,1], yticks=[0,1],xlim =x_lim,ylim=y_lim)\n",
    "    # g.set(xticks=[0,1], yticks=[0,1] )\n",
    "    g.map_upper(sns.kdeplot, levels=4, color=\".2\")\n",
    "    g.map_upper(sns.scatterplot,s=8)\n",
    "    # g.map_upper(sns.kdeplot)\n",
    "    # g.map_upper(hexbin)\n",
    "    # g.map_upper( sns.scatterplot, hue=None, levels=5, color=\".2\")\n",
    "    # sns.displot(data=penguins, x=\"flipper_length_mm\", kde=True)\n",
    "    g.map_diag(sns.histplot,  bins=15)\n",
    "    # g.map_upper(generate_text)\n",
    "    g.tick_params(direction='out', labelsize = 'large' ,   grid_alpha=0.8)\n",
    "    g.savefig(picname)\n",
    "xvarall=['angle_m', 'angle_s', 'area_m', 'area_s', 'csd_m', 'csd_s', 'dis_m',\n",
    "       'dis_s', 'per_m', 'per_s', 'zsc_m', 'zsc_s']\n",
    "x_vars = []\n",
    "xva_std=[name for name in xvarall if '_s' in name ]\n",
    "xva_men=[name for name in xvarall if '_m' in name ]\n",
    "\n",
    "yva_std=[name for name in xvarall if '_s' in name ] +['f1']\n",
    "yva_men=[name for name in xvarall if '_m' in name ] +['f1']\n",
    "\n",
    "picname ='ss_scatter_norm.png'\n",
    "sactterplot(dataset,picname,xva_std,yva_std) \n",
    "print(picname,' complete')\n",
    "picname ='mm_scatter_norm.png'\n",
    "sactterplot(dataset,picname,xva_men,yva_men) \n",
    "print(picname,' complete')\n",
    "\n",
    "picname ='sm_scatter_norm.png'\n",
    "sactterplot(dataset,picname,xva_std,xva_men)\n",
    "print(picname,' complete')\n",
    "\n",
    "# g.set(   font_scale=2   )\n",
    "# p=so.Plot(dataset_log).pair( y= ['precisions'],x=df_normalized_log.columns[0:2] )\n",
    "# p=so.Plot(dataset).pair( y= ['precisions'],x=df_normalized.columns[0:1])\n",
    " \n",
    "# for index,axs  in enumerate( g.axes )  :\n",
    "#     for ax in axs:\n",
    "#         print(index,ax.get_xlabel())\n",
    "#         print(index,ax.get_ylabel())\n",
    "#     break\n",
    "# p.layout(size =(9,9))\n",
    "# p .pair( y= ['precisions'],x=['area_mean'  ]  ) .layout(engine=\"tight\")\n",
    "# p .add(so.Dots())\n",
    "# so.Plot(dataset[\"precisions\"], dataset[df_normalized.columns[0 ]]).add(so.Bar())\n",
    "# )b\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " std 与f1之间的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因子分析 kmo验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_bartlett_sphericity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chi_square_value,p_value\u001b[39m=\u001b[39mcalculate_bartlett_sphericity(df_normalized)\n\u001b[1;32m      2\u001b[0m chi_square_value, p_value  \n\u001b[1;32m      3\u001b[0m \u001b[39m# 计算KMO值 \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_bartlett_sphericity' is not defined"
     ]
    }
   ],
   "source": [
    "chi_square_value,p_value=calculate_bartlett_sphericity(df_normalized)\n",
    "chi_square_value, p_value  \n",
    "# 计算KMO值 \n",
    "kmo_all,kmo_model=calculate_kmo(df_normalized)\n",
    "print(kmo_model)\n",
    "# # 读取数据\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# # 初始化因子分析模型，并指定要提取的因子数量\n",
    "# fa = FactorAnalyzer(n_factors=3)\n",
    "\n",
    "# # 使用最大似然方法对数据进行因子分析\n",
    "# fa.fit(data)\n",
    "\n",
    "# # 输出因子载荷矩阵\n",
    "# loadings = fa.loadings_\n",
    "# print(loadings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算因子载荷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "import numpy as np\n",
    "# 读取数据\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# 初始化因子分析模型，并使用最大似然方法对数据进行因子分析\n",
    "fa = FactorAnalyzer(  n_factors=5, rotation='varimax')\n",
    "fa.fit(df_normalized)\n",
    "# 输出因子载荷矩阵\n",
    "loadings = fa.loadings_\n",
    "loadings[np.where(loadings<0.52)]=0.0\n",
    "loadings =np.round(loadings,3)\n",
    "loading_df =pd.DataFrame(loadings,index= df_normalized.columns)\n",
    "loading_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angle_m</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_m</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_m</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_m</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_m</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_m</th>\n",
       "      <td>0.922</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4\n",
       "angle_m  0.802  0.521  0.000  0.000  0.000\n",
       "angle_s  0.000  0.860  0.000  0.000  0.000\n",
       "area_m   0.000  0.000  0.000  0.927  0.000\n",
       "area_s   0.000  0.000  0.000  0.000  0.988\n",
       "csd_m    0.000  0.000  0.820  0.000  0.000\n",
       "csd_s    0.000  0.000  0.858  0.000  0.000\n",
       "dis_m    0.882  0.000  0.000  0.000  0.000\n",
       "dis_s    0.000  0.886  0.000  0.000  0.000\n",
       "per_m    0.920  0.000  0.000  0.000  0.000\n",
       "per_s    0.000  0.889  0.000  0.000  0.000\n",
       "zsc_m    0.922  0.000  0.000  0.000  0.000\n",
       "zsc_s    0.000  0.750  0.000  0.000  0.000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算因子得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(fa.transform(df_normalized))\n",
    "df_score##得到公因子\n",
    "dataset_all = pd.concat([df_score,data_y['f1s_norm']],axis=1)\n",
    "dataset_all.to_csv('score_factor.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>f1s_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.035171</td>\n",
       "      <td>-2.907823</td>\n",
       "      <td>-1.755299</td>\n",
       "      <td>-2.604062</td>\n",
       "      <td>-4.361072</td>\n",
       "      <td>0.451423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.106282</td>\n",
       "      <td>-2.897774</td>\n",
       "      <td>-1.799173</td>\n",
       "      <td>-2.644531</td>\n",
       "      <td>-4.375469</td>\n",
       "      <td>0.415303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.154561</td>\n",
       "      <td>-2.990766</td>\n",
       "      <td>-1.764143</td>\n",
       "      <td>-2.608986</td>\n",
       "      <td>-4.342817</td>\n",
       "      <td>0.436324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.158804</td>\n",
       "      <td>-3.129805</td>\n",
       "      <td>-1.721912</td>\n",
       "      <td>-2.506251</td>\n",
       "      <td>-4.317761</td>\n",
       "      <td>0.440437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.171845</td>\n",
       "      <td>-3.019863</td>\n",
       "      <td>-1.755372</td>\n",
       "      <td>-2.567805</td>\n",
       "      <td>-4.307640</td>\n",
       "      <td>0.428147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10795</th>\n",
       "      <td>-1.976529</td>\n",
       "      <td>2.240060</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>0.190922</td>\n",
       "      <td>-7.241791</td>\n",
       "      <td>0.493026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>-1.424303</td>\n",
       "      <td>1.039697</td>\n",
       "      <td>1.105403</td>\n",
       "      <td>1.104223</td>\n",
       "      <td>-5.744873</td>\n",
       "      <td>0.503362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797</th>\n",
       "      <td>-0.360594</td>\n",
       "      <td>-1.284060</td>\n",
       "      <td>2.218452</td>\n",
       "      <td>2.253462</td>\n",
       "      <td>-3.986175</td>\n",
       "      <td>0.500425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10798</th>\n",
       "      <td>1.306840</td>\n",
       "      <td>-5.312971</td>\n",
       "      <td>5.062557</td>\n",
       "      <td>4.805549</td>\n",
       "      <td>0.737204</td>\n",
       "      <td>0.506204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>1.546588</td>\n",
       "      <td>-6.076082</td>\n",
       "      <td>6.527436</td>\n",
       "      <td>5.559649</td>\n",
       "      <td>3.158873</td>\n",
       "      <td>0.474330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10800 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4  f1s_norm\n",
       "0      6.035171 -2.907823 -1.755299 -2.604062 -4.361072  0.451423\n",
       "1      6.106282 -2.897774 -1.799173 -2.644531 -4.375469  0.415303\n",
       "2      6.154561 -2.990766 -1.764143 -2.608986 -4.342817  0.436324\n",
       "3      6.158804 -3.129805 -1.721912 -2.506251 -4.317761  0.440437\n",
       "4      6.171845 -3.019863 -1.755372 -2.567805 -4.307640  0.428147\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "10795 -1.976529  2.240060  0.224787  0.190922 -7.241791  0.493026\n",
       "10796 -1.424303  1.039697  1.105403  1.104223 -5.744873  0.503362\n",
       "10797 -0.360594 -1.284060  2.218452  2.253462 -3.986175  0.500425\n",
       "10798  1.306840 -5.312971  5.062557  4.805549  0.737204  0.506204\n",
       "10799  1.546588 -6.076082  6.527436  5.559649  3.158873  0.474330\n",
       "\n",
       "[10800 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_all \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算累计方差矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.84914474, 3.65995627, 1.87227449, 1.2124545 , 1.07459808,\n",
       "        0.12345267]),\n",
       " array([0.32076206, 0.30499636, 0.15602287, 0.10103787, 0.08954984,\n",
       "        0.01028772]),\n",
       " array([0.32076206, 0.62575842, 0.78178129, 0.88281917, 0.97236901,\n",
       "        0.98265673]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fa.get_factor_variance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到特征值ev、特征向量v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.50185504e+00, 1.51691552e+00, 1.28329556e+00, 8.81856838e-01,\n",
       "       5.91151845e-01, 1.20640322e-01, 7.09542395e-02, 1.39205257e-02,\n",
       "       7.09188108e-03, 6.23357706e-03, 4.32069233e-03, 1.76395533e-03])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到特征值ev、特征向量v\n",
    "ev,v=fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.48842212e+00,  1.22784474e+00,  9.96711220e-01,  4.78214541e-01,\n",
       "        1.49675739e-01,  1.29348597e-02,  2.13791408e-03, -6.51173241e-03,\n",
       "       -2.97086965e-02, -4.30665520e-02, -2.46582461e-01, -3.53188123e-01])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算不包含area的因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angle_m', 'angle_s', 'csd_m', 'csd_s', 'dis_m', 'dis_s', 'per_m',\n",
       "       'per_s', 'zsc_m', 'zsc_s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized.columns\n",
    "name =['angle_m', 'angle_s',  'csd_m', 'csd_s', 'dis_m',\n",
    "       'dis_s', 'per_m', 'per_s', 'zsc_m', 'zsc_s']\n",
    "df_noarea = df_normalized[name]\n",
    "df_noarea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.7122173557400223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dls/anaconda3/envs/pytorch2.0/lib/python3.10/site-packages/factor_analyzer/utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 因子分析检验\n",
    "_,p_value=calculate_bartlett_sphericity(df_normalized)\n",
    "print( p_value)  \n",
    "# 计算KMO值 \n",
    "_,kmo_model=calculate_kmo(df_normalized)\n",
    "print(kmo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angle_m</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_m</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_m</th>\n",
       "      <td>0.862</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_m</th>\n",
       "      <td>0.909</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_m</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2    3    4\n",
       "angle_m  0.806  0.513  0.000  0.0  0.0\n",
       "angle_s  0.000  0.879  0.000  0.0  0.0\n",
       "csd_m    0.000  0.000  0.842  0.0  0.0\n",
       "csd_s    0.000  0.000  0.905  0.0  0.0\n",
       "dis_m    0.862  0.000  0.000  0.0  0.0\n",
       "dis_s    0.000  0.861  0.000  0.0  0.0\n",
       "per_m    0.909  0.000  0.000  0.0  0.0\n",
       "per_s    0.000  0.880  0.000  0.0  0.0\n",
       "zsc_m    0.923  0.000  0.000  0.0  0.0\n",
       "zsc_s    0.000  0.779  0.000  0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "import numpy as np\n",
    "# 读取数据\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# 初始化因子分析模型，并使用最大似然方法对数据进行因子分析\n",
    "fa_noarea = FactorAnalyzer(  n_factors=5, rotation='varimax')\n",
    "fa_noarea.fit(df_noarea)\n",
    "# 输出因子载荷矩阵\n",
    "loadings = fa_noarea.loadings_\n",
    "loadings[np.where(loadings<0.5)]=0.0\n",
    "loadings =np.round(loadings,3)\n",
    "loading_df =pd.DataFrame(loadings,index= df_noarea.columns)\n",
    "loading_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.68950014, 3.64449147, 2.07996566, 0.26226281, 0.09568801]),\n",
       " array([0.36895001, 0.36444915, 0.20799657, 0.02622628, 0.0095688 ]),\n",
       " array([0.36895001, 0.73339916, 0.94139573, 0.96762201, 0.97719081]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa_noarea.get_factor_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fa1', 'fa2', 'fa3', 'fa4', 'fa5'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score_noarea = pd.DataFrame(fa_noarea.transform(df_noarea),columns=['fa1','fa2','fa3','fa4','fa5',])\n",
    "df_score_noarea.columns##得到公因子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_noarea = pd.concat([df_score_noarea[['fa1','fa2','fa3']],data_y['f1s_norm']],axis=1)\n",
    "dataset_all_noarea.to_csv('score_factor_noarea.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算带ir的 相关关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namefilter ['angle_irmean', 'angle_irstd', 'area_irmean', 'area_irstd', 'csd_irmean', 'csd_irstd', 'dis_irmean', 'dis_irstd', 'per_irmean', 'per_irstd', 'zsc_irmean', 'zsc_irstd']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "# 计算巴特利特P值\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "path = '/home/dls/data/openmmlab/test_RandomForestRegressor/large_samples_b60_all.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4D_6_process.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4H_6_process.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4T_6_process.csv'\n",
    "# path = '/home/dls/data/openmmlab/test_RandomForestRegressor/merge_nopre_test.csv'\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "data = pd.read_csv(path)\n",
    "data_y=['recalls',\n",
    " 'recalls_norm',\n",
    " 'accs',\n",
    " 'accs_norm',\n",
    " 'f1s',\n",
    " 'f1s_norm',\n",
    " 'precisions',\n",
    " 'precisions_norm']\n",
    "x_name = [name for name in data.columns if name not in data_y]\n",
    "data_x =  data[x_name]\n",
    "data_y =  data[data_y]\n",
    "# print(data_x.head())\n",
    "# print(data_y.head())\n",
    "\n",
    "# need_name = ['area','per','zsc','disminmax','disavg','angleminmax','angleavg','csdminmax','csdavg','precisions' ]\n",
    "# data =data[need_name]\n",
    "# 将数据集分成训练集和测试集 最后一列是因变量 剩余的列是自变量\n",
    "scaler = MinMaxScaler() #为了使用同一个归一化器 先归一化 再分割\n",
    "df_normalized_ir = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns)\n",
    "# df_normalized.head()\n",
    "\n",
    "corr = data_x.corrwith(data_y['f1s_norm'] )\n",
    "with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_x_norm_ir.txt','w') as f :\n",
    "    ind =corr.index.str.contains('_ir')\n",
    "    # out =corr[ind]\n",
    "    # ind1 = out.index.str.contains('norm')\n",
    "    # out =out[~ind1]\n",
    "    f.write(\n",
    "        f'Correlation between data_x and f1s_norm :\\n{corr[ind]}\\n'\n",
    "    )\n",
    "\n",
    "# 发现norm完后的结果普遍变好 利用spherephd处理的结果没有普通的好\n",
    "\n",
    "corr1 = df_normalized_ir.corrwith(data_y['f1s_norm'] )\n",
    "with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_norm_ir.txt','w') as f :\n",
    "    ind =corr1.index.str.contains('_ir')\n",
    "    # out =corr[ind]\n",
    "    # ind1 = out.index.str.contains('norm')\n",
    "    # out =out[ind1]\n",
    "    f.write(\n",
    "        f'Correlation between df_normalized and f1s_norm :\\n{corr1[ind]}\\n'\n",
    "    )\n",
    "namefilter =[name for name in df_normalized_ir.columns if (('norm' not in name) and ('_ir' in name)) ]\n",
    "print('namefilter',namefilter)\n",
    "df_normalized_ir = df_normalized_ir[namefilter]##loc 去除方差行 选择多列和多行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_normalized_mean.columns Index(['angle_m', 'area_m', 'csd_m', 'dis_m', 'per_m', 'zsc_m'], dtype='object')\n",
      "df_normalized_std.columns Index(['angle_s', 'area_s', 'csd_s', 'dis_s', 'per_s', 'zsc_s'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# dataset = pd.concat([df_normalized.iloc[:,0:2],data_y['f1s_norm']],axis=1)\n",
    "\n",
    "# newcolumns = []\n",
    "df_normalized_mean = pd.DataFrame()\n",
    "df_normalized_std = pd.DataFrame()\n",
    "\n",
    "for name ,value in df_normalized_ir.items():\n",
    "    # print(name)\n",
    "    if 'std'in name :\n",
    "        base=name.split('_')[0]\n",
    "        newname = base+'_s'\n",
    "        df_normalized_std[newname] =value\n",
    "    elif 'mean'in name :\n",
    "        base=name.split('_')[0]\n",
    "        newname = base+'_m'\n",
    "        df_normalized_mean[newname] =value\n",
    "    else:\n",
    "        raise(f'name is {name}')\n",
    " \n",
    "# print('df_normalized_ir.columns',df_normalized_ir.columns)\n",
    "print('df_normalized_mean.columns',df_normalized_mean.columns)\n",
    "print('df_normalized_std.columns',df_normalized_std.columns)\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关关系出图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# sns.set_style(\"whitegrid\") # 设置seaborn样式\n",
    "plt.rc('text', usetex=True) # 启用LaTeX\n",
    "plt.rc('font', family='sans-serif') # 设置LaTeX字体\n",
    "sns.set(  font_scale=2.5 )\n",
    "\n",
    "dataset = pd.concat([df_normalized_mean,data_y['f1s_norm']],axis=1)\n",
    "def qqplot (x, y, **kwargs):\n",
    "   regax =sns.regplot(x=x,y=y,line_kws={\"color\": \"k\",\"linewidth\": 0.8, \"linestyle\": \"--\"}, scatter_kws={\"s\": 2   })##,marker=\"*\" , \"color\": \"k\"\n",
    "def hexbin(x, y, color, **kwargs):\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)\n",
    "def upper_regplot(x, y, **kwargs):\n",
    "      scaax =sns.kdeplot(x=x,y=y  )\n",
    "def generate_text(x,y, **kwargs):\n",
    "      scaax = plt.gca()\n",
    "      x_lim = (-0.05,1.1)\n",
    "      x_range = scaax.get_xlim()[1] - scaax.get_xlim()[0]\n",
    "      y_range = scaax.get_ylim()[1] - scaax.get_ylim()[0]\n",
    "      value = round(abs(x.corr(y)), 2)\n",
    "      if value >0.3:\n",
    "            color='r'\n",
    "            fontweight='heavy'\n",
    "      else:\n",
    "            color=None\n",
    "            fontweight='normal'\n",
    "\n",
    "      text = r\"$\\rho$\"+  f\"={round(x.corr(y), 2)}\"\n",
    "      # plt.gca().set_color(p1.get_color())\n",
    "      plt.text(x_lim[1]*0.65 ,x_lim[1]*0.65 , text, ha='center', va='center',fontsize =30,color=color,fontweight=fontweight)\n",
    "g = sns.PairGrid(dataset )\n",
    "g.map_lower(qqplot)\n",
    "g.map_lower(generate_text)\n",
    "x_lim = (-0.2,1.1)\n",
    "y_lim = (-0.2,1.1)\n",
    "g.set(xticks=[0,1], yticks=[0,1],xlim =x_lim,ylim=y_lim)\n",
    "g.map_upper(sns.kdeplot, levels=4, color=\".2\")\n",
    "g.map_upper(sns.scatterplot,s=8)\n",
    " \n",
    "g.map_diag(sns.histplot,  bins=15)\n",
    "# g.map_upper(generate_text)\n",
    "g.tick_params(direction='out', labelsize = 'large' ,   grid_alpha=0.8)\n",
    "# g.savefig('./scatter.png')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = pd.concat([df_normalized_std,data_y['f1s_norm']],axis=1)\n",
    "g_std = sns.PairGrid(dataset_std )\n",
    "g_std.map_lower(qqplot)\n",
    "g_std.map_lower(generate_text)\n",
    "x_lim = (-0.2,1.1)\n",
    "y_lim = (-0.2,1.1)\n",
    "g_std.set(xticks=[0,1], yticks=[0,1],xlim =x_lim,ylim=y_lim)\n",
    "g_std.map_upper(sns.kdeplot, levels=4, color=\".2\")\n",
    "g_std.map_upper(sns.scatterplot,s=8)\n",
    " \n",
    "g_std.map_diag(sns.histplot,  bins=15)\n",
    "# g.map_upper(generate_text)\n",
    "g_std.tick_params(direction='out', labelsize = 'large' ,   grid_alpha=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af733b2acf3a54772a457f53cfd6bc2da8b445ac6aa3b9c2f225c3427b4e9c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
