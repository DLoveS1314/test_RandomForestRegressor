{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算每个格网的几何平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean/FULLER4D_6_gmean.csv\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean/FULLER4H_6_gmean.csv\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean/FULLER4T_6_gmean.csv\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean/ISEA4D_6_gmean.csv\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean/ISEA4H_6_gmean.csv\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean/ISEA4T_6_gmean.csv\n"
     ]
    }
   ],
   "source": [
    "outpath= '/home/dls/data/openmmlab/test_RandomForestRegressor/g_mean'\n",
    "path = '/home/dls/data/openmmlab/DGGRID/src/apps/caldgg/c'\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import gmean  \n",
    "res= 6\n",
    "for prj , tpo in product(proj,topo):\n",
    "    dirname = os.path.join(path,f\"{prj}{tpo}_{res}_all.csv\")\n",
    "    outname = os.path.join(outpath,f\"{prj}{tpo}_{res}_gmean.csv\")\n",
    "    print(outname)\n",
    "    df =pd.read_csv(dirname)\n",
    "    # break\n",
    "    df.drop( columns=['seqnum'],inplace=True)\n",
    "    # print(df.columns)\n",
    "\n",
    "    result = gmean(df,axis=0)\n",
    "    # print(result)\n",
    "    dfout =pd.DataFrame({f'{name}_gmean':[result[i] ]for i, name in enumerate(df.columns) })\n",
    "    # print(dfout.head())\n",
    "    dfout.to_csv(outname,index=False)\n",
    "    # df_gmean = np.log(df/result) \n",
    "    # print(df_gmean.sum())\n",
    "    # break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始计算错误了 把变异系数计算成了方差 进行纠正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out'\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "res =6\n",
    "import pandas as pd\n",
    "import os ,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.join(path,f\"{proj[0]}{topo[0]}_{res}_all.csv\")\n",
    "df =pd.read_csv(dirname)\n",
    "# 筛选包含 mean 或 std 的列 默认filter列筛选 想行筛选 坐标轴选择 axis=0\n",
    "columns_with_mean = df.filter(like='_mean').columns\n",
    "columns_with_std = df.filter(like='_std').columns\n",
    "\n",
    "#  遍历这些列，计算对应的 mean/std 的值\n",
    "# iteritems（） - 遍历列（键，值）对\n",
    "# iterrows（） - 遍历行（索引，序列）对\n",
    "for column_name, column_data in df[columns_with_mean].iteritems():#遍历每一列\n",
    "    stat_name = column_name.split('_')[0]  # 获取统计名称，如 area\n",
    "    std_column_name = f'{stat_name}_std'  # 构造对应的 std 列名，如 area_std\n",
    "    if std_column_name in columns_with_std:#如果有对应std名\n",
    "        std_column_data = df[std_column_name]  # 获取对应的 std 列数据\n",
    "        df[f'{stat_name}_cv'] = column_data / std_column_data  # 计算 mean/std 的值\n",
    "df = df.loc[:,~df.columns.str.contains('_var')]##loc 去除方差行 选择多列和多行\n",
    "df = df.loc[:,~df.columns.str.contains('_std')]##loc 去除标准差行\n",
    "df = df.loc[:,~df.columns.str.contains('_cv')]##loc 去除mean\n",
    "# 输出结果\n",
    "# print(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df[sorted(df.columns)]\n",
    "df_sorted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop= df_sorted.drop(['name', 'r_lon', 'r_lat','recalls','accs','f1s'], axis=1 )\n",
    "pre =df_drop.pop('precisions')\n",
    "df_drop['precisions'] =pre\n",
    "df_drop.head()\n",
    "outpath = '/home/dls/data/openmmlab/test_RandomForestRegressor/processdata'\n",
    "dirname = os.path.join(outpath,f\"{proj[0]}{topo[0]}_{res}_process.csv\")\n",
    "df_drop.to_csv(dirname,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_scaler(path,scaler):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "def load_scaler(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "        return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 读取原始数据\n",
    "df=df_drop\n",
    "\n",
    "# 对数据进行归一化 本身就是逐列归一化\n",
    "scaler = MinMaxScaler()\n",
    "scaler_path = os.path.join(outpath,f\"{proj[0]}{topo[0]}_{res}_scaler.pkl\")\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "save_scaler(scaler_path,scaler)\n",
    "sacler = load_scaler (scaler_path)\n",
    "# 将归一化后的数据再反归一化为原来的值\n",
    "df_denormalized = pd.DataFrame(scaler.inverse_transform(df_normalized), columns=df.columns)\n",
    "# 将 MinMaxScaler 对象保存到文件中\n",
    "\n",
    "# 打印结果\n",
    "print('Original data:\\n', df.head())\n",
    "print('Normalized data:\\n', df_normalized.head())\n",
    "print('Denormalized data:\\n', df_denormalized.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取在0 0 位置的 测试集精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "res =6\n",
    "json_path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out_0_0'\n",
    "json_dirname = os.path.join(json_path,f\"{proj[0]}{topo[0]}_l{res}_00.json\")\n",
    "json_dirname=json_dirname.lower()\n",
    "# 打开JSON文件并加载数据\n",
    "# 读取JSON文件\n",
    "with open(json_dirname, 'r') as file:\n",
    "    json_str = file.read()\n",
    "monitor ={'accs': 'accuracy/top1','precisions':'single-label/precision','recalls':'single-label/recall','f1s':'single-label/f1-score' }\n",
    "# 将JSON转换为字典\n",
    "jsondata = json.loads(json_str)\n",
    "# for key,value in monitor.items():\n",
    "#     current_score = jsondata[value]  \n",
    "dictval ={ key:[jsondata[value]] for key,value in monitor.items() }\n",
    "# 打印字典\n",
    "# print(dictval)\n",
    "dftestval =pd.DataFrame.from_dict(dictval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4H_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4H_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4T_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4T_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4D_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4D_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4H_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4H_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/FULLER4D_6_all.csv,/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4T_6_metrics.csv\n",
      "save result to /home/dls/data/openmmlab/mmclassification/tools/pandas_save/out/ISEA4T_6_all.csv\n"
     ]
    }
   ],
   "source": [
    "# 如果改变了统计量 需要重新合并/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out 里的 calstatis 和 metrics 文件变为all文件 与savepad功能相同\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out'\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "for prj , tpo in product(proj,topo):\n",
    "    \n",
    "    dirname_calstatis = os.path.join(path,f\"{prj}{tpo}_{res}_calstatis.csv\")\n",
    "    dirname_metircs = os.path.join(path,f\"{prj}{tpo}_{res}_metrics.csv\")\n",
    "    dirname_all = os.path.join(path,f\"{prj}{tpo}_{res}_all.csv\")\n",
    "    print(f'dirname: {dirname},{dirname_metircs}')\n",
    "    df_cal =pd.read_csv(dirname_calstatis)\n",
    "    df_met =pd.read_csv(dirname_metircs)\n",
    "    merged_df = pd.merge(df_cal, df_met, on='name', how='inner')\n",
    "    print(f'save result to {dirname_all}')\n",
    "    merged_df.to_csv(dirname_all,index=False)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理mmcls生成的数据 去除不需要的列 结果保存成 _process后缀的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out/FULLER4D_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out/FULLER4H_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out/FULLER4T_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out/ISEA4D_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out/ISEA4H_6_all.csv\n",
      "dirname: /home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out/ISEA4T_6_all.csv\n"
     ]
    }
   ],
   "source": [
    "# path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out'\n",
    "path = '/home/dls/data/openmmlab/mmclassification/tools/large_samples_b60/out'\n",
    "outpath = '/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60'\n",
    "json_path = '/home/dls/data/openmmlab/mmclassification/tools/pandas_save/out_0_0'\n",
    "\n",
    "proj = ['FULLER','ISEA']\n",
    "topo = ['4D','4H','4T']\n",
    "res =6\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import product\n",
    "import json\n",
    "# 处理json数据为df文件\n",
    "def gettestscore(json_dirname):\n",
    "    with open(json_dirname, 'r') as file:\n",
    "        json_str = file.read()\n",
    "    monitor ={'accs': 'accuracy/top1','precisions':'single-label/precision','recalls':'single-label/recall','f1s':'single-label/f1-score' }\n",
    "    # 将JSON转换为字典\n",
    "    jsondata = json.loads(json_str)\n",
    "    # for key,value in monitor.items():\n",
    "    #     current_score = jsondata[value]  \n",
    "    dictval ={ key:[jsondata[value]] for key,value in monitor.items() }\n",
    "    # 打印字典\n",
    "    # print(dictval)\n",
    "    dftestval =pd.DataFrame.from_dict(dictval)\n",
    "    return dftestval\n",
    " \n",
    "for prj , tpo in product(proj,topo):\n",
    "    \n",
    "    dirname = os.path.join(path,f\"{prj}{tpo}_{res}_all.csv\")\n",
    "    print(f'dirname: {dirname}')\n",
    "    df =pd.read_csv(dirname)\n",
    "    # 筛选包含 mean 或 std 的列 默认filter列筛选 想行筛选 坐标轴选择 axis=0\n",
    "    columns_with_mean = df.filter(like='_mean').columns\n",
    "    columns_with_std = df.filter(like='_std').columns\n",
    "    #  遍历这些列，计算对应的 mean/std 的值\n",
    "    # iteritems（） - 遍历列（键，值）对\n",
    "    # iterrows（） - 遍历行（索引，序列）对\n",
    "    for column_name, column_data in df[columns_with_mean].items ():#遍历每一列\n",
    "        stat_name = column_name.split('_')[0]  # 获取统计名称，如 area\n",
    "        std_column_name = f'{stat_name}_std'  # 构造对应的 std 列名，如 area_std\n",
    "        if std_column_name in columns_with_std:#如果有对应std名\n",
    "            std_column_data = df[std_column_name]  # 获取对应的 std 列数据\n",
    "            df[f'{stat_name}_cv'] = std_column_data / column_data     # 计算 mean/std 的值\n",
    "    ##删除多余的变量\n",
    "\n",
    "    # df_drop= df_sorted.drop(['name', 'r_lon', 'r_lat','recalls','accs','f1s'], axis=1 )\n",
    "    df_drop= df.drop(['name', 'r_lon', 'r_lat' ], axis=1 )\n",
    "\n",
    "\n",
    "    # 调整 precisions 的位置\n",
    "    # preci =df_drop.pop('precisions')\n",
    "    # print(preci.head())\n",
    "    json_dirname = os.path.join(json_path,f\"{prj}{tpo}_l{res}_00.json\")\n",
    "    json_dirname=json_dirname.lower()\n",
    "    pre_test = gettestscore(json_dirname)#0 0 位置的预测精度\n",
    "\n",
    "    csv_dirname = os.path.join(json_path,\"out\",f\"{prj}{tpo}_{res}_00_calstatis.csv\") #0 0 位置处的格网几何属性\n",
    "    metric_test = pd.read_csv(csv_dirname)\n",
    "    metric_test= metric_test.drop(['name', 'r_lon', 'r_lat' ], axis=1 )\n",
    "    for key ,val in metric_test.items():##因为涉及负值 所以用变化率来表示 abs((a-b)/a)\n",
    "        value = float(metric_test[key]) \n",
    "        df_drop[f'{key}_norm'] = abs( (value-df_drop[key])/value ) ##几何属性数据标准化一下 存成对应的norm数据 利用的是 0 0 位置处进行标准化\n",
    "    for key ,val in pre_test.items():\n",
    "        value = float(pre_test[key]) \n",
    "        # print(key ,value)\n",
    "        df_drop[f'{key}_norm'] = abs( (value-df_drop[key])/value )##预测精度数据标准化一下 存成对应的norm数据 利用的是 0 0 位置处进行标准化\n",
    "        # print(df_drop[f'{key}_norm'])\n",
    "    # 根据计算的 mean_norm  计算mean_norm的std\n",
    "    # for column_name, column_data in df_drop.items ():#遍历每一列\n",
    "    #     if 'mean_norm' in column_name:##只有一个值了 无法计算std\n",
    "    #         stat_name = column_name.split('_')[0]  # 获取统计名称，如 area\n",
    "    #         df_drop[f'{stat_name}_meanstd'] =  column_data.std()     # 计算 mean/std 的值\n",
    "    # df_drop['precisions_norm'] = df_drop['precisions']/float(pre_test['precisions']) ##计算相应的归一化\n",
    "    # df_drop['f1s_norm']  = df_drop['f1s']/float(pre_test['f1s']) ##计算相应的归一化\n",
    "    # df_drop['accs_norm']  = df_drop['accs']/float(pre_test['accs']) ##计算相应的归一化\n",
    "    # df_drop['recalls_norm']  = df_drop['recalls']/float(pre_test['recalls']) ##计算相应的归一化\n",
    "    # print(pre_test['precisions'] )\n",
    "    # 再次排序\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('_var')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('_cv')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('max')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('min')]##loc 去除方差行 选择多列和多行\n",
    "    df_drop = df_drop.loc[:,~df_drop.columns.str.contains('maxmin')]##loc 去除方差行 选择多列和多行\n",
    "\n",
    "\n",
    "    # df = df.loc[:,~df.columns.str.contains('_std')]##loc 去除标准差行\n",
    "    # df = df.loc[:,~df.columns.str.contains('_cv')]##loc 去除mean\n",
    "    # df_sorted = df[sorted(df.columns)]\n",
    "    df_drop = df_drop[sorted(df_drop.columns)]\n",
    "    # print(preci_norm[preci_norm>0.8])\n",
    " \n",
    "    # df_drop.head()\n",
    "    # 输出处理好的数据\n",
    "    os.makedirs(outpath,   exist_ok=True)\n",
    "    outdirname = os.path.join(outpath,f\"{prj}{tpo}_{res}_process.csv\")\n",
    "    df_drop.to_csv(outdirname,index=False)\n",
    "    # print(df_drop.head())\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终合并所有的csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60/FULLER4T_6_process.csv\n",
      "(1800, 56)\n",
      "(1800, 56)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60/ISEA4H_6_process.csv\n",
      "(1800, 56)\n",
      "(3600, 56)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60/FULLER4D_6_process.csv\n",
      "(1800, 56)\n",
      "(5400, 56)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60/FULLER4H_6_process.csv\n",
      "(1800, 56)\n",
      "(7200, 56)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60/ISEA4T_6_process.csv\n",
      "(1800, 56)\n",
      "(9000, 56)\n",
      "/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/large_samples_b60/ISEA4D_6_process.csv\n",
      "(1800, 56)\n",
      "(10800, 56)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 读取文件夹中所有 CSV 文件\n",
    "folder_path = outpath\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 创建空的 DataFrame\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# 循环遍历所有 CSV 文件\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    print(file_path)\n",
    "    # 读取 CSV 文件并将数据合并到 DataFrame 中\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(df.shape)\n",
    "\n",
    "    merged_df = pd.concat([merged_df, df], axis=0)\n",
    "    print(merged_df.shape)\n",
    "merge_file = 'large_samples_b60_all.csv'\n",
    "# 将合并后的 DataFrame 写入新的 CSV 文件\n",
    "merged_df.to_csv( merge_file , index=False)\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['accs', 'accs_norm', 'angle_irmean', 'angle_irmean_norm', 'angle_irstd',\n",
       "       'angle_irstd_norm', 'angle_mean', 'angle_mean_norm', 'angle_std',\n",
       "       'angle_std_norm', 'area_irmean', 'area_irmean_norm', 'area_irstd',\n",
       "       'area_irstd_norm', 'area_mean', 'area_mean_norm', 'area_std',\n",
       "       'area_std_norm', 'csd_irmean', 'csd_irmean_norm', 'csd_irstd',\n",
       "       'csd_irstd_norm', 'csd_mean', 'csd_mean_norm', 'csd_std',\n",
       "       'csd_std_norm', 'dis_irmean', 'dis_irmean_norm', 'dis_irstd',\n",
       "       'dis_irstd_norm', 'dis_mean', 'dis_mean_norm', 'dis_std',\n",
       "       'dis_std_norm', 'f1s', 'f1s_norm', 'per_irmean', 'per_irmean_norm',\n",
       "       'per_irstd', 'per_irstd_norm', 'per_mean', 'per_mean_norm', 'per_std',\n",
       "       'per_std_norm', 'precisions', 'precisions_norm', 'recalls',\n",
       "       'recalls_norm', 'zsc_irmean', 'zsc_irmean_norm', 'zsc_irstd',\n",
       "       'zsc_irstd_norm', 'zsc_mean', 'zsc_mean_norm', 'zsc_std',\n",
       "       'zsc_std_norm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_df.columns)\n",
    "merged_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据归一化测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor  ,GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 加载数据集\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/merge_all.csv'\n",
    "# data = pd.read_csv(path)\n",
    "data =merged_df\n",
    "# print(list(data.columns))\n",
    "# data_y_temp = ['recalls','accs','f1s','precisions']\n",
    "# data_y = []\n",
    "# for name in data_y_temp:\n",
    "#     data_y.append(name)\n",
    "#     data_y.append(name+'_norm')\n",
    "data_y=['recalls',\n",
    " 'recalls_norm',\n",
    " 'accs',\n",
    " 'accs_norm',\n",
    " 'f1s',\n",
    " 'f1s_norm',\n",
    " 'precisions',\n",
    " 'precisions_norm']\n",
    "x_name = [name for name in data.columns if name not in data_y]\n",
    " \n",
    "# data_x =  data.iloc[:, :-1]\n",
    "# data_y =  data.iloc[:,  -1]\n",
    "\n",
    "# # print(data_x.head())\n",
    "# # print(data_y.head())\n",
    "\n",
    "# # need_name = ['area','per','zsc','disminmax','disavg','angleminmax','angleavg','csdminmax','csdavg','precisions' ]\n",
    "# # data =data[need_name]\n",
    "# # 将数据集分成训练集和测试集 最后一列是因变量 剩余的列是自变量\n",
    "# scaler = MinMaxScaler()\n",
    "# df_normalized = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_normalized, data_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因子分析实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "# 计算巴特利特P值\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算相关关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angle_m', 'angle_s', 'area_m', 'area_s', 'csd_m', 'csd_s', 'dis_m',\n",
       "       'dis_s', 'per_m', 'per_s', 'zsc_m', 'zsc_s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/dls/data/openmmlab/test_RandomForestRegressor/large_samples_b60_all.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4D_6_process.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4H_6_process.csv'\n",
    "# path ='/home/dls/data/openmmlab/test_RandomForestRegressor/processdata/FULLER4T_6_process.csv'\n",
    "# path = '/home/dls/data/openmmlab/test_RandomForestRegressor/merge_nopre_test.csv'\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "data = pd.read_csv(path)\n",
    "data_y=['recalls',\n",
    " 'recalls_norm',\n",
    " 'accs',\n",
    " 'accs_norm',\n",
    " 'f1s',\n",
    " 'f1s_norm',\n",
    " 'precisions',\n",
    " 'precisions_norm']\n",
    "x_name = [name for name in data.columns if name not in data_y]\n",
    "data_x =  data[x_name]\n",
    "data_y =  data[data_y]\n",
    "# print(data_x.head())\n",
    "# print(data_y.head())\n",
    "\n",
    "# need_name = ['area','per','zsc','disminmax','disavg','angleminmax','angleavg','csdminmax','csdavg','precisions' ]\n",
    "# data =data[need_name]\n",
    "# 将数据集分成训练集和测试集 最后一列是因变量 剩余的列是自变量\n",
    "scaler = MinMaxScaler() #为了使用同一个归一化器 先归一化 再分割\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(data_x), columns=data_x.columns)\n",
    "# df_normalized.head()\n",
    "\n",
    "corr = data_x.corrwith(data_y['f1s_norm'] )\n",
    "with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_x_norm.txt','w') as f :\n",
    "    ind =corr.index.str.contains('_ir')\n",
    " \n",
    "    f.write(\n",
    "        f'Correlation between data_x and f1s_norm :\\n{corr[~ind]}\\n'\n",
    "    )\n",
    "\n",
    "# 发现norm完后的结果普遍变好 利用spherephd处理的结果没有普通的好\n",
    "\n",
    "corr = df_normalized.corrwith(data_y['f1s_norm'] )\n",
    "with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_norm.txt','w') as f :\n",
    "    ind =corr.index.str.contains('_ir')\n",
    "    out =corr[~ind]\n",
    "    ind1 = out.index.str.contains('norm')\n",
    "    out =out[ind1]\n",
    "    f.write(\n",
    "        f'Correlation between df_normalized and f1s_norm :\\n{out}\\n'\n",
    "    )\n",
    "# 发现归一化与不归一化结果相同 但是归一化后 对随机森林方便验证\n",
    "\n",
    "#提取包含norm的值作为自变量\n",
    "\n",
    "namefilter =[name for name in df_normalized.columns if (('norm'in name) and ('_ir' not in name)) ]\n",
    "\n",
    "df_normalized = df_normalized[namefilter]##loc 去除方差行 选择多列和多行\n",
    "# newcolumns = []\n",
    "df_normalized_temp = pd.DataFrame()\n",
    "\n",
    "for name ,value in df_normalized.items():\n",
    "    if 'std'in name :\n",
    "        base=name.split('_')[0]\n",
    "        newname = base+'_s'\n",
    "        df_normalized_temp[newname] =value\n",
    "    elif 'mean'in name :\n",
    "        base=name.split('_')[0]\n",
    "        newname = base+'_m'\n",
    "        df_normalized_temp[newname] =value\n",
    "    else:\n",
    "        raise(f'name is {name}')\n",
    "df_normalized =df_normalized_temp\n",
    "df_normalized.columns\n",
    "# 发现norm完后的结果普遍变好 利用spherephd处理的结果没有普通的好\n",
    "# corr = df_normalized.corrwith(data_y['precisions_norm'] )\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_norm.txt','w') as f :\n",
    "#     f.write(f'Correlation between df_normalized and  precisions_norm  :\\n{corr}\\n')\n",
    "\n",
    "# corr = df_normalized.corrwith(data_y['precisions'] )\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm_y.txt','w') as f :\n",
    "#     f.write(f'Correlation between df_normalized and  precisions  :\\n{corr}\\n')\n",
    "# for key ,val in data_y.items():\n",
    "#     corr = data_x.corrwith(data_y[key] )\n",
    "#     with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_x.txt','w') as f :\n",
    "#         f.write(f'Correlation between data_x and {key}:\\n{corr}\\n')\n",
    "#         # print(f'Correlation between x and {key}:', corr)\n",
    "# with open('/home/dls/data/openmmlab/test_RandomForestRegressor/log_norm.txt','w+') as f :\n",
    "#     for key ,val in data_y.items():\n",
    "#         corr = df_normalized.corrwith(data_y[key] )\n",
    "#         f.write(f'Correlation between df_normalized and {key}:\\n{corr}\\n')\n",
    "#         # print(f'Correlation between data_x and {key}:', corr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 自变量之间的相关关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "          angle_m   angle_s    area_m    area_s     csd_m     csd_s     dis_m  \\\n",
      "angle_m  1.000000  0.733481  0.127207 -0.099987  0.571825  0.517703  0.916474   \n",
      "angle_s  0.733481  1.000000  0.358206 -0.206168  0.701869  0.492228  0.675619   \n",
      "area_m   0.127207  0.358206  1.000000 -0.243147  0.362679  0.420878  0.264534   \n",
      "area_s  -0.099987 -0.206168 -0.243147  1.000000 -0.148510 -0.132433 -0.133378   \n",
      "csd_m    0.571825  0.701869  0.362679 -0.148510  1.000000  0.934670  0.659728   \n",
      "csd_s    0.517703  0.492228  0.420878 -0.132433  0.934670  1.000000  0.681538   \n",
      "dis_m    0.916474  0.675619  0.264534 -0.133378  0.659728  0.681538  1.000000   \n",
      "dis_s    0.701709  0.873451  0.194253  0.217481  0.684538  0.473138  0.577868   \n",
      "per_m    0.917082  0.613128  0.158212 -0.111890  0.617530  0.650230  0.989923   \n",
      "per_s    0.815268  0.947343  0.245615 -0.062839  0.691717  0.501824  0.718261   \n",
      "zsc_m    0.903050  0.559148 -0.052486 -0.061313  0.567189  0.578541  0.943023   \n",
      "zsc_s    0.698069  0.855103  0.544420 -0.082999  0.686230  0.578597  0.673840   \n",
      "\n",
      "            dis_s     per_m     per_s     zsc_m     zsc_s  \n",
      "angle_m  0.701709  0.917082  0.815268  0.903050  0.698069  \n",
      "angle_s  0.873451  0.613128  0.947343  0.559148  0.855103  \n",
      "area_m   0.194253  0.158212  0.245615 -0.052486  0.544420  \n",
      "area_s   0.217481 -0.111890 -0.062839 -0.061313 -0.082999  \n",
      "csd_m    0.684538  0.617530  0.691717  0.567189  0.686230  \n",
      "csd_s    0.473138  0.650230  0.501824  0.578541  0.578597  \n",
      "dis_m    0.577868  0.989923  0.718261  0.943023  0.673840  \n",
      "dis_s    1.000000  0.538205  0.920914  0.523044  0.799954  \n",
      "per_m    0.538205  1.000000  0.676008  0.973084  0.606547  \n",
      "per_s    0.920914  0.676008  1.000000  0.647082  0.904702  \n",
      "zsc_m    0.523044  0.973084  0.647082  1.000000  0.506493  \n",
      "zsc_s    0.799954  0.606547  0.904702  0.506493  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# 自变量之间的相关关系\n",
    "corr_matrix = df_normalized.corr()\n",
    "\n",
    "print('Correlation matrix:')\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10800, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10800, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关关系出图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# sns.set_style(\"whitegrid\") # 设置seaborn样式\n",
    "plt.rc('text', usetex=True) # 启用LaTeX\n",
    "plt.rc('font', family='sans-serif') # 设置LaTeX字体\n",
    "sns.set(  font_scale=2.5 )\n",
    "# dataset = pd.concat([df_normalized.iloc[:,0:2],data_y['f1s_norm']],axis=1)\n",
    "dataset = pd.concat([df_normalized,data_y['f1s_norm']],axis=1)\n",
    "\n",
    "print(dataset.shape)\n",
    "def qqplot (x, y, **kwargs):\n",
    "   regax =sns.regplot(x=x,y=y,line_kws={\"color\": \"k\",\"linewidth\": 0.8, \"linestyle\": \"--\"}, scatter_kws={\"s\": 2   })##,marker=\"*\" , \"color\": \"k\"\n",
    "#    x_lim = (-0.1,1.0)\n",
    "#    y_lim = (0,1.0)\n",
    "#    regax.set_xlim(x_lim)\n",
    "#    regax.set_ylim(y_lim)\n",
    "   # regax.set_facecolor(\"none\")\n",
    "def hexbin(x, y, color, **kwargs):\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hexbin(x, y, gridsize=25, cmap=cmap, **kwargs)\n",
    "def upper_regplot(x, y, **kwargs):\n",
    "      # rgb = np.array([216, 229, 210])/255.0\n",
    "      scaax =sns.kdeplot(x=x,y=y  )\n",
    "      # x_lim = (-0.1,1.1)\n",
    "      # y_lim = (-0.1,1.1)\n",
    "      # scaax.set_xlim(x_lim)\n",
    "      # scaax.set_ylim(y_lim)\n",
    "def generate_text(x,y, **kwargs):\n",
    "      # print(kwargs)\n",
    "      scaax = plt.gca()\n",
    "      # print(scaax .yaxis.get_label())\n",
    "\n",
    "      x_lim = (-0.05,1.1)\n",
    "      # scaax.set_xlim(x_lim)\n",
    "      # scaax.set_ylim(y_lim)\n",
    "      x_range = scaax.get_xlim()[1] - scaax.get_xlim()[0]\n",
    "      y_range = scaax.get_ylim()[1] - scaax.get_ylim()[0]\n",
    "      value = round(x.corr(y), 2)\n",
    "      if value >0.4:\n",
    "            color='r'\n",
    "            fontweight='heavy'\n",
    "      else:\n",
    "            color=None\n",
    "            fontweight='normal'\n",
    "\n",
    "      text = r\"$\\rho$\"+  f\"={round(x.corr(y), 2)}\"\n",
    "      # plt.gca().set_color(p1.get_color())\n",
    "      plt.text(x_lim[1]*0.65 ,x_lim[1]*0.65 , text, ha='center', va='center',fontsize =30,color=color,fontweight=fontweight)\n",
    "      \n",
    "# 定义一个函数用于在上半部分绘制文本\n",
    "# def plot_text(x,y,  **kwargs):\n",
    "#       # scaax=ax  \n",
    "#       # rgb = np.array([216, 229, 210])/255.0\n",
    "\n",
    "#       # sns.kdeplot(x=x,y=y,  levels=4, color=\".2\")\n",
    "#       r= np.random.randint(0,10)\n",
    " \n",
    "#       # x_range = scaax.get_xlim()[1] - scaax.get_xlim()[0]\n",
    "#       # y_range = scaax.get_ylim()[1] - scaax.get_ylim()[0]\n",
    "#       # x_lim = (-0.05,1.0)\n",
    "#       # y_lim = (-0.05,1.0)\n",
    "#       # scaax.set_xlim(x_lim)\\\n",
    "#       # scaax.set_ylim(y_lim)\n",
    "#       plt.text(0.5, 0.5, text, ha='center', va='center',fontsize = 2)\n",
    "     \n",
    "#       # scaax.patch.set_facecolor(rgb)\n",
    "#       # scaax.patch.set_alpha(0.50)\n",
    "#       # sns.despine(ax=scaax, left=True, bottom=True)\n",
    "\n",
    "# 在上半部分应用绘图函数\n",
    "\n",
    "g = sns.PairGrid(dataset )\n",
    "# g.map_upper(sns.regplot)\n",
    "# reg =sns.regplot(marker=\"*\",line_kws={\"color\": \"C1\"})\n",
    "g.map_lower(qqplot)\n",
    "g.map_lower(generate_text)\n",
    "\n",
    "x_lim = (-0.2,1.1)\n",
    "y_lim = (-0.2,1.1)\n",
    "# g.map_upper(upper_regplot)\n",
    "g.set(xticks=[0,1], yticks=[0,1],xlim =x_lim,ylim=y_lim)\n",
    "# g.set(xticks=[0,1], yticks=[0,1] )\n",
    "g.map_upper(sns.kdeplot, levels=4, color=\".2\")\n",
    "g.map_upper(sns.scatterplot,s=8)\n",
    "# g.map_upper(sns.kdeplot)\n",
    "# g.map_upper(hexbin)\n",
    "# g.map_upper( sns.scatterplot, hue=None, levels=5, color=\".2\")\n",
    "# sns.displot(data=penguins, x=\"flipper_length_mm\", kde=True)\n",
    "g.map_diag(sns.histplot,  bins=15)\n",
    "# g.map_upper(generate_text)\n",
    "g.tick_params(direction='out', labelsize = 'large' ,   grid_alpha=0.8)\n",
    "g.savefig('./scatter.png')\n",
    "# g.set(   font_scale=2   )\n",
    "# p=so.Plot(dataset_log).pair( y= ['precisions'],x=df_normalized_log.columns[0:2] )\n",
    "# p=so.Plot(dataset).pair( y= ['precisions'],x=df_normalized.columns[0:1])\n",
    " \n",
    "# for index,axs  in enumerate( g.axes )  :\n",
    "#     for ax in axs:\n",
    "#         print(index,ax.get_xlabel())\n",
    "#         print(index,ax.get_ylabel())\n",
    "#     break\n",
    "# p.layout(size =(9,9))\n",
    "# p .pair( y= ['precisions'],x=['area_mean'  ]  ) .layout(engine=\"tight\")\n",
    "# p .add(so.Dots())\n",
    "# so.Plot(dataset[\"precisions\"], dataset[df_normalized.columns[0 ]]).add(so.Bar())\n",
    "# )b\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因子分析 kmo验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7122173557400223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dls/anaconda3/envs/pytorch2.0/lib/python3.10/site-packages/factor_analyzer/utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "chi_square_value,p_value=calculate_bartlett_sphericity(df_normalized)\n",
    "chi_square_value, p_value  \n",
    "# 计算KMO值 \n",
    "kmo_all,kmo_model=calculate_kmo(df_normalized)\n",
    "print(kmo_model)\n",
    "# # 读取数据\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# # 初始化因子分析模型，并指定要提取的因子数量\n",
    "# fa = FactorAnalyzer(n_factors=3)\n",
    "\n",
    "# # 使用最大似然方法对数据进行因子分析\n",
    "# fa.fit(data)\n",
    "\n",
    "# # 输出因子载荷矩阵\n",
    "# loadings = fa.loadings_\n",
    "# print(loadings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算因子载荷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "import numpy as np\n",
    "# 读取数据\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# 初始化因子分析模型，并使用最大似然方法对数据进行因子分析\n",
    "fa = FactorAnalyzer(  n_factors=5, rotation='varimax')\n",
    "fa.fit(df_normalized)\n",
    "# 输出因子载荷矩阵\n",
    "loadings = fa.loadings_\n",
    "loadings[np.where(loadings<0.52)]=0.0\n",
    "loadings =np.round(loadings,3)\n",
    "loading_df =pd.DataFrame(loadings,index= df_normalized.columns)\n",
    "loading_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angle_m</th>\n",
       "      <td>0.802</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_m</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_m</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_m</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_m</th>\n",
       "      <td>0.920</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_m</th>\n",
       "      <td>0.922</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2      3      4\n",
       "angle_m  0.802  0.521  0.000  0.000  0.000\n",
       "angle_s  0.000  0.860  0.000  0.000  0.000\n",
       "area_m   0.000  0.000  0.000  0.927  0.000\n",
       "area_s   0.000  0.000  0.000  0.000  0.988\n",
       "csd_m    0.000  0.000  0.820  0.000  0.000\n",
       "csd_s    0.000  0.000  0.858  0.000  0.000\n",
       "dis_m    0.882  0.000  0.000  0.000  0.000\n",
       "dis_s    0.000  0.886  0.000  0.000  0.000\n",
       "per_m    0.920  0.000  0.000  0.000  0.000\n",
       "per_s    0.000  0.889  0.000  0.000  0.000\n",
       "zsc_m    0.922  0.000  0.000  0.000  0.000\n",
       "zsc_s    0.000  0.750  0.000  0.000  0.000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loading_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算因子得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score = pd.DataFrame(fa.transform(df_normalized))\n",
    "df_score##得到公因子\n",
    "dataset_all = pd.concat([df_score,data_y['f1s_norm']],axis=1)\n",
    "dataset_all.to_csv('score_factor.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>f1s_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.035171</td>\n",
       "      <td>-2.907823</td>\n",
       "      <td>-1.755299</td>\n",
       "      <td>-2.604062</td>\n",
       "      <td>-4.361072</td>\n",
       "      <td>0.451423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.106282</td>\n",
       "      <td>-2.897774</td>\n",
       "      <td>-1.799173</td>\n",
       "      <td>-2.644531</td>\n",
       "      <td>-4.375469</td>\n",
       "      <td>0.415303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.154561</td>\n",
       "      <td>-2.990766</td>\n",
       "      <td>-1.764143</td>\n",
       "      <td>-2.608986</td>\n",
       "      <td>-4.342817</td>\n",
       "      <td>0.436324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.158804</td>\n",
       "      <td>-3.129805</td>\n",
       "      <td>-1.721912</td>\n",
       "      <td>-2.506251</td>\n",
       "      <td>-4.317761</td>\n",
       "      <td>0.440437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.171845</td>\n",
       "      <td>-3.019863</td>\n",
       "      <td>-1.755372</td>\n",
       "      <td>-2.567805</td>\n",
       "      <td>-4.307640</td>\n",
       "      <td>0.428147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10795</th>\n",
       "      <td>-1.976529</td>\n",
       "      <td>2.240060</td>\n",
       "      <td>0.224787</td>\n",
       "      <td>0.190922</td>\n",
       "      <td>-7.241791</td>\n",
       "      <td>0.493026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10796</th>\n",
       "      <td>-1.424303</td>\n",
       "      <td>1.039697</td>\n",
       "      <td>1.105403</td>\n",
       "      <td>1.104223</td>\n",
       "      <td>-5.744873</td>\n",
       "      <td>0.503362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10797</th>\n",
       "      <td>-0.360594</td>\n",
       "      <td>-1.284060</td>\n",
       "      <td>2.218452</td>\n",
       "      <td>2.253462</td>\n",
       "      <td>-3.986175</td>\n",
       "      <td>0.500425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10798</th>\n",
       "      <td>1.306840</td>\n",
       "      <td>-5.312971</td>\n",
       "      <td>5.062557</td>\n",
       "      <td>4.805549</td>\n",
       "      <td>0.737204</td>\n",
       "      <td>0.506204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>1.546588</td>\n",
       "      <td>-6.076082</td>\n",
       "      <td>6.527436</td>\n",
       "      <td>5.559649</td>\n",
       "      <td>3.158873</td>\n",
       "      <td>0.474330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10800 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4  f1s_norm\n",
       "0      6.035171 -2.907823 -1.755299 -2.604062 -4.361072  0.451423\n",
       "1      6.106282 -2.897774 -1.799173 -2.644531 -4.375469  0.415303\n",
       "2      6.154561 -2.990766 -1.764143 -2.608986 -4.342817  0.436324\n",
       "3      6.158804 -3.129805 -1.721912 -2.506251 -4.317761  0.440437\n",
       "4      6.171845 -3.019863 -1.755372 -2.567805 -4.307640  0.428147\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "10795 -1.976529  2.240060  0.224787  0.190922 -7.241791  0.493026\n",
       "10796 -1.424303  1.039697  1.105403  1.104223 -5.744873  0.503362\n",
       "10797 -0.360594 -1.284060  2.218452  2.253462 -3.986175  0.500425\n",
       "10798  1.306840 -5.312971  5.062557  4.805549  0.737204  0.506204\n",
       "10799  1.546588 -6.076082  6.527436  5.559649  3.158873  0.474330\n",
       "\n",
       "[10800 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_all \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算累计方差矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.84914474, 3.65995627, 1.87227449, 1.2124545 , 1.07459808,\n",
       "        0.12345267]),\n",
       " array([0.32076206, 0.30499636, 0.15602287, 0.10103787, 0.08954984,\n",
       "        0.01028772]),\n",
       " array([0.32076206, 0.62575842, 0.78178129, 0.88281917, 0.97236901,\n",
       "        0.98265673]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fa.get_factor_variance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到特征值ev、特征向量v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.50185504e+00, 1.51691552e+00, 1.28329556e+00, 8.81856838e-01,\n",
       "       5.91151845e-01, 1.20640322e-01, 7.09542395e-02, 1.39205257e-02,\n",
       "       7.09188108e-03, 6.23357706e-03, 4.32069233e-03, 1.76395533e-03])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到特征值ev、特征向量v\n",
    "ev,v=fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.48842212e+00,  1.22784474e+00,  9.96711220e-01,  4.78214541e-01,\n",
       "        1.49675739e-01,  1.29348597e-02,  2.13791408e-03, -6.51173241e-03,\n",
       "       -2.97086965e-02, -4.30665520e-02, -2.46582461e-01, -3.53188123e-01])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算不包含area的因子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['angle_m', 'angle_s', 'csd_m', 'csd_s', 'dis_m', 'dis_s', 'per_m',\n",
       "       'per_s', 'zsc_m', 'zsc_s'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized.columns\n",
    "name =['angle_m', 'angle_s',  'csd_m', 'csd_s', 'dis_m',\n",
    "       'dis_s', 'per_m', 'per_s', 'zsc_m', 'zsc_s']\n",
    "df_noarea = df_normalized[name]\n",
    "df_noarea.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.7122173557400223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dls/anaconda3/envs/pytorch2.0/lib/python3.10/site-packages/factor_analyzer/utils.py:244: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 因子分析检验\n",
    "_,p_value=calculate_bartlett_sphericity(df_normalized)\n",
    "print( p_value)  \n",
    "# 计算KMO值 \n",
    "_,kmo_model=calculate_kmo(df_normalized)\n",
    "print(kmo_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>angle_m</th>\n",
       "      <td>0.806</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_m</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csd_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_m</th>\n",
       "      <td>0.862</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dis_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_m</th>\n",
       "      <td>0.909</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_m</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsc_s</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0      1      2    3    4\n",
       "angle_m  0.806  0.513  0.000  0.0  0.0\n",
       "angle_s  0.000  0.879  0.000  0.0  0.0\n",
       "csd_m    0.000  0.000  0.842  0.0  0.0\n",
       "csd_s    0.000  0.000  0.905  0.0  0.0\n",
       "dis_m    0.862  0.000  0.000  0.0  0.0\n",
       "dis_s    0.000  0.861  0.000  0.0  0.0\n",
       "per_m    0.909  0.000  0.000  0.0  0.0\n",
       "per_s    0.000  0.880  0.000  0.0  0.0\n",
       "zsc_m    0.923  0.000  0.000  0.0  0.0\n",
       "zsc_s    0.000  0.779  0.000  0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "import numpy as np\n",
    "# 读取数据\n",
    "# data = pd.read_csv('data.csv')\n",
    "\n",
    "# 初始化因子分析模型，并使用最大似然方法对数据进行因子分析\n",
    "fa_noarea = FactorAnalyzer(  n_factors=5, rotation='varimax')\n",
    "fa_noarea.fit(df_noarea)\n",
    "# 输出因子载荷矩阵\n",
    "loadings = fa_noarea.loadings_\n",
    "loadings[np.where(loadings<0.5)]=0.0\n",
    "loadings =np.round(loadings,3)\n",
    "loading_df =pd.DataFrame(loadings,index= df_noarea.columns)\n",
    "loading_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.68950014, 3.64449147, 2.07996566, 0.26226281, 0.09568801]),\n",
       " array([0.36895001, 0.36444915, 0.20799657, 0.02622628, 0.0095688 ]),\n",
       " array([0.36895001, 0.73339916, 0.94139573, 0.96762201, 0.97719081]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa_noarea.get_factor_variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fa1', 'fa2', 'fa3', 'fa4', 'fa5'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_score_noarea = pd.DataFrame(fa_noarea.transform(df_noarea),columns=['fa1','fa2','fa3','fa4','fa5',])\n",
    "df_score_noarea.columns##得到公因子\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all_noarea = pd.concat([df_score_noarea[['fa1','fa2','fa3']],data_y['f1s_norm']],axis=1)\n",
    "dataset_all_noarea.to_csv('score_factor_noarea.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af733b2acf3a54772a457f53cfd6bc2da8b445ac6aa3b9c2f225c3427b4e9c3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
